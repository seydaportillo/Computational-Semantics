# -*- coding: utf-8 -*-
"""Exercise 11-12

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TLSoh8WPSbeiit92y6RTMPevNkVeX6k4
"""

#Exercise 11

import nltk
from nltk.corpus import wordnet as wn
nltk.download('punkt')
nltk.download('wordnet')
nltk.download("averaged_perceptron_tagger")

# Collect (by hand) a small corpus of 3 example sentences of varying lengths from any newspaper or magazine in a language that
# 1) you are reasonably proficient in,
# 2) is preferably not English,
# and 3) is covered in the Open Multilingual WordNet data available in NLTK.
sentence1 = "Taylor Swift and Travis Kelce departed the Kansas City Chiefs vs. New York Jets game separately on Sunday night, according to reports."
sentence2 = "Itâ€™s unusual to see a major New York contemporary art museum host an exhibition so shaped by this religion and its spiritual aims."
sentence3 = "Catharina Coenen is a first-generation German immigrant to Pennsylvania, where she teaches biology at Allegheny College."

corpus = [sentence1, sentence2, sentence3]
open_class_words = ["noun", "verb", "adjective", "adverb"]

def is_open_class_word(word):
    synsets = wn.synsets(word, lang='eng')
    if not synsets:
        return False
    pos = synsets[0].pos()
    return pos in ('n', 'v', 'r', 'j')

for i, sentence in enumerate(corpus, start=1):
    print(f"Sentence {i}: {sentence}")

    tokens = nltk.word_tokenize(sentence)
    open_class_word_count = 0
    for word in tokens:
        if is_open_class_word(word):
            print(f"- Open-class word: {word}")
            synsets = wn.synsets(word, lang='eng')
            if synsets:
                print("Synsets:")
                for j, synset in enumerate(synsets, start=1):
                    print(f"- Sense {j}: {synset.name()} {synset.definition()}")
            else:
                print('No synsets found')
            open_class_word_count += 1

    print(f"Total open-class words in Sentence {i}: {open_class_word_count}\n")

    print()

#Using WordNet, determine how many senses there are for each of the open-class words in each sentence.
def count_synsets(sentence):
    tokens = nltk.word_tokenize(sentence)
    synset_counts = {token: len(wn.synsets(token.lower())) for token, pos_tag in nltk.pos_tag(tokens) if pos_tag[0] in "NVRJ"}
    return synset_counts


for i, sentence in enumerate(corpus, start=1):
    print(f"Sentence {i}:")
    synset_counts = count_synsets(sentence)
    for word, count in synset_counts.items():
        print(f"{word}: {count} synsets")

    print()

#How many distinct combinations of senses are there for each sentence?
#How does this number seem to vary with sentence length?

[print(f'Total number of words in Sentence{i+1} is', len(sentence.split())) for i, sentence in enumerate([sentence1, sentence2, sentence3])]

def total_sense_combinations(sentence):
    tokens = nltk.word_tokenize(sentence)
    open_class_words = [word for word in tokens if is_open_class_word(word)]
    senses_per_word = []

    for word in open_class_words:
        synsets = wn.synsets(word, lang='eng')
        if synsets:
            senses_per_word.append(len(synsets))
        else:
            senses_per_word.append(0)

    total_combinations = 1
    for count in senses_per_word:
        total_combinations *= count

    return total_combinations

for i, sentence in enumerate([sentence1, sentence2, sentence3], start=1):
    print(f"Sentence {i}: {sentence}")
    print(f"Total sense combinations: {total_sense_combinations(sentence)}\n")

#12  [Adapted from J&M, ex. in chapter 23] Using WordNet, tag each open-class word in your mini-corpus with its correct sense tag (synset).

"""I couldn't manage this one."""